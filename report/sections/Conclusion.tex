\section*{Conclusion}

In summary, the exploration of n-Gram language models using Good-Turing and Linear Interpolation on "Ulysses" and "Pride and Prejudice" datasets revealed clear trends in model performance. Linear Interpolation consistently outperformed Good-Turing, displaying lower perplexity scores, indicating better generalization and reduced confusion when predicting sentences.

Interestingly, "Pride and Prejudice" exhibited lower perplexity scores overall compared to "Ulysses," suggesting a smoother predictability in the former's language patterns.
